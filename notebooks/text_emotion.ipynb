!pip install -qU ipywidgets
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
from transformers import DistilBertTokenizer, TFDistilBertModel, create_optimizer
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow as tf

print("Num GPUs Available:", len(tf.config.list_physical_devices('GPU')))
# Ensure TensorFlow sees the GPU
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))

# XLA Compilation
tf.config.optimizer.set_jit(True)
# Load dataset
train_ds = pd.read_csv('/kaggle/input/sentiment-analysis-dataset/train.csv', encoding='latin1')
val_ds = pd.read_csv('/kaggle/input/sentiment-analysis-dataset/test.csv', encoding='latin1')

train_ds = train_ds[['text', 'sentiment']].dropna()
val_ds = val_ds[['text', 'sentiment']].dropna()

sentiment_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}
train_ds['sentiment'] = train_ds['sentiment'].map(sentiment_mapping)
val_ds['sentiment'] = val_ds['sentiment'].map(sentiment_mapping)
# Tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', use_fast=True)

# Create TF datasets (dynamic padding)
def encode_texts(texts, labels, batch_size=32, shuffle=False):
    encodings = tokenizer(
        list(texts),
        truncation=True,
        padding=True,  # dynamic padding
        return_tensors="tf"
    )
    dataset = tf.data.Dataset.from_tensor_slices(({
        'input_ids': encodings['input_ids'],
        'attention_mask': encodings['attention_mask']
    }, labels))
    if shuffle:
        dataset = dataset.shuffle(buffer_size=10000)
    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)
    return dataset

BATCH_SIZE = 32
train_dataset = encode_texts(train_ds['text'], np.array(train_ds['sentiment']), 
                             batch_size=BATCH_SIZE, shuffle=True)
val_dataset = encode_texts(val_ds['text'], np.array(val_ds['sentiment']), 
                           batch_size=BATCH_SIZE, shuffle=False)
# Mixed Precision
tf.keras.mixed_precision.set_global_policy('mixed_float16')

# Model
bert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')

def build_model():
    input_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_ids')
    attention_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='attention_mask')
    
    bert_output = bert_model(input_ids, attention_mask=attention_mask)[0]
    cls_token = bert_output[:, 0, :]  
    dropout = tf.keras.layers.Dropout(0.3)(cls_token)
    output = tf.keras.layers.Dense(3, dtype='float32', activation='softmax')(dropout)
    
    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)
    return model

model = build_model()
num_train_steps = int(len(train_ds) / BATCH_SIZE * 5)  # for 5 epochs
optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01
)

model.compile(optimizer=optimizer,
              loss=SparseCategoricalCrossentropy(),
              metrics=['accuracy'])

model.summary()
early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)

history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=20,
    callbacks=[early_stopping]
)

# Plot
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('DistilBERT Model Accuracy')
plt.show()
val_preds = model.predict(val_dataset)
val_labels = np.argmax(val_preds, axis=1)
print(classification_report(val_ds['sentiment'], val_labels, target_names=['negative','neutral','positive']))
# Save the trained model in TensorFlow SavedModel format
model.save("text_sentiment_model")

# Save the model in HDF5 format (for TensorFlow Serving)
model.save("text_sentiment_model.h5")

print("Model saved successfully!")
